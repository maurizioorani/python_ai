# Exploring Local Document Chat with Llama 3.2 (Streamlit Frontend, Embedchain Backend)

This application is part of an ongoing exploration of different technologies and frameworks to create an efficient and user-friendly tool for interacting with local documents. In this iteration, we leverage **Streamlit** for building an intuitive web-based frontend and **Embedchain** as the backend framework to handle document embedding, storage, and retrieval. This setup allows you to chat with your local PDF, text, and markdown files using the powerful Llama 3.2 language model, running locally via Ollama.

You can upload documents, add them to a knowledge base managed by Embedchain, and then ask questions that are relevant to their content. The application uses Embedchain to create embeddings of your documents and your questions, finding relevant information to inform the responses generated by the local Llama 3.2 model. Streamlit provides a seamless interface for this interaction, including document preview and management capabilities.

## How it Works

1.  **Document Upload:** You can upload PDF, text, or markdown files through the sidebar in the Streamlit interface.
2.  **Knowledge Base Integration:** When you upload a file and click "Add to Knowledge Base," the application uses Embedchain to process the file content, generate embeddings (numerical representations of the text), and store them in a local Chroma vector database. Each document is assigned a unique ID, and its metadata (filename, type, etc.) is tracked.
3.  **Chat Interface:** The "Chat Interface" tab provides a Streamlit chat window where you can ask questions related to the content of the documents you've added.
4.  **Question Answering:** When you ask a question, the application utilizes Embedchain to:
    * Embed your question using the same embedding model (Llama 3.2 via Ollama).
    * Search the Chroma vector database for the most semantically similar document chunks based on their embeddings.
    * Send these relevant document chunks and your question to the Llama 3.2 model running on Ollama.
    * Receive a generated answer from Llama 3.2 that is contextually relevant to your documents.
5.  **Document Management:** The "Document Management" tab, built with Streamlit components, displays a list of all documents currently in your Embedchain-managed knowledge base. It offers options to delete individual documents or clear the entire knowledge base.
6.  **Document Preview:** Before adding a file to the knowledge base, Streamlit allows you to preview its content directly in the sidebar. For text and markdown files, there's also a Streamlit button to process the entire content with the local LLM for summarization or analysis.
7.  **Chat History:** Streamlit's session state management keeps track of your conversation history within the current session, providing context for subsequent questions.

## Use Cases

This application, built with Streamlit and Embedchain, is useful for various scenarios where you need to interact with local documents:

* **Research:** Quickly find answers and insights from research papers, reports, and articles.
* **Document Analysis:** Understand the content of lengthy documents without extensive reading.
* **Note Review:** Chat with your personal notes and markdown files to recall specific information.
* **Learning:** Ask questions about study materials, textbooks, and other educational resources.
* **Personal Knowledge Management:** Create a localized knowledge base for your files.

## Installation

Follow these steps to install and run the application:

### Prerequisites

1.  **Python 3.8 or higher:** Ensure Python is installed. Check your version with `python --version` or `python3 --version`.
2.  **Ollama:** This application requires Ollama to run the Llama 3.2 language model locally. Install it from the [official Ollama website](https://ollama.com/).
3.  **Llama 3.2 Model:** After installing Ollama, pull the Llama 3.2 model by running in your terminal:
    ```bash
    ollama pull llama3.2:latest
    ```

### Installation Steps

1.  **Clone Repository (Optional):** If the code is in a GitHub repository:
    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```
2.  **Install Dependencies:** Navigate to the directory with the Python code and install the necessary libraries using pip:
    ```bash
    pip install streamlit embedchain streamlit-chat python-dotenv
    ```

### Running the Application

1.  **Save the Code:** Save the provided Python code as a `.py` file (e.g., `app.py`).
2.  **Run Streamlit:** Open your terminal in the directory where you saved `app.py` and run:
    ```bash
    streamlit run app.py
    ```
    The application will open in your web browser.

## Usage

1.  **Open the Application:** The Streamlit application will open in your browser.
2.  **Upload Documents:** In the sidebar of the "Chat Interface," select the file type and upload your file.
3.  **Preview File:** The file content will be displayed in the sidebar.
4.  **Add to Knowledge Base:** Click "Add to Knowledge Base" to process the file with Embedchain.
5.  **Chat:** In the main "Chat Interface," ask questions related to your documents.
6.  **Manage Documents:** Go to the "Document Management" tab to view and delete documents from the Embedchain knowledge base.
7.  **Clear Chat History:** Use the button in the "Chat Interface" to clear the conversation.

## Requirements and Dependencies

* **Python:** 3.8+
* **Ollama:** Running locally with the `llama3.2:latest` model.
* **Python Libraries:**
    * `streamlit`
    * `embedchain`
    * `streamlit-chat`
    * `python-dotenv` (optional)

Ensure Ollama is running for the application to connect to Llama 3.2. Verify that the `llama3.2:latest` model has been pulled in Ollama.