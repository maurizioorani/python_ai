# ğŸ¤–ğŸ§  AI PROJECTS - Python, LangChain and Local LLMs

This project demonstrates applications showcasing interaction with a local **Ollama** instance ğŸ¦™ for various AI tasks using **Python** and **LangChain**.

## âœ¨ Applications

### 1. RAG System with Local LLM (Ollama) and ChromaDB (`RAG_example.py`) ğŸ“šğŸ”

This project implements a **Retrieval-Augmented Generation (RAG)** system using:
* A local Large Language Model (LLM) served by **Ollama** ğŸ¦™
* A **Chroma** vector database ğŸ’¾ for document storage and retrieval
* A **Gradio** web interface ğŸŒ for interaction.

It allows you to upload your documents (PDF, TXT, MD), ask questions about their content, and receive answers generated by a local LLM, ensuring your data stays **private** ğŸ”’. The system also keeps track of your chat history ğŸ•’.

#### Features

* ğŸ”’ **Local Processing:** Uses Ollama to run LLMs locally, keeping your data private.
* ğŸ“¤ **Document Upload:** Supports uploading PDF, TXT, and Markdown (.md) files.
* ğŸ’¾ **Vector Storage:** Uses ChromaDB to store document embeddings for efficient retrieval.
* ğŸ’¬ **Chat Interface:** Provides a user-friendly Gradio interface to chat with your documents.
* ğŸ“š **Source Citation:** Displays the source documents used to generate the answer.
* ğŸ”¢ **Token Count:** Shows the number of tokens used in the user query and the assistant's response.
* ğŸ•’ **Chat History:** Saves and loads conversation history (also stored in ChromaDB).
* âš™ï¸ **Management:** Allows listing stored documents, deleting all documents, and deleting chat history.

#### Prerequisites âœ…

* Python 3.8+ ğŸ
* `pip` (Python package installer) ğŸ“¦
* Git (for cloning the repository) ğŸ™
* [Ollama](https://ollama.com/) installed and running ğŸ¦™ğŸ’¨.

#### Setup Instructions âš™ï¸

1.  **Clone the Repository:** ğŸ’»
    ```bash
    git clone <your-repository-url>
    cd <your-repository-directory>
    ```

2.  **Create a Virtual Environment (Recommended):** ğŸŒ±
    ```bash
    # Linux/macOS
    python3 -m venv venv
    source venv/bin/activate

    # Windows
    python -m venv venv
    .\venv\Scripts\activate
    ```

3.  **Create the Requirements File (`RAG_requirements.txt`):** ğŸ“„
    Create a file named `RAG_requirements.txt` in the project directory with the following content:
    ```txt
    gradio
    langchain
    langchain-chroma
    langchain-huggingface
    langchain-openai # Used for Ollama's OpenAI-compatible API
    tiktoken
    pypdf
    unstructured[md] # Installs markdown parsing extras for unstructured
    sentence-transformers # For HuggingFaceEmbeddings
    chromadb >= 0.4.20 # Ensure a recent version for compatibility
    ollama # Optional, but useful for managing Ollama via command line
    ```
    *Note: You might need to adjust versions based on compatibility.*

4.  **Install Dependencies:** â¬‡ï¸
    Ensure your virtual environment is active, then run:
    ```bash
    pip install -r RAG_requirements.txt
    ```
    Alternatively, if an `environment.yml` is provided for Conda:
    ```bash
    conda env create -f environment.yml # Or conda install --file environment.yml
    ```

5.  **Setup Ollama:** ğŸ¦™
    * **Install Ollama:** Follow instructions on the [Ollama website](https://ollama.com/).
    * **Run Ollama:** Ensure the Ollama service is running (e.g., run the installed application or use `ollama serve` in the terminal).
    * **Verify Ollama:** Check if Ollama is running by opening a terminal and typing:
        ```bash
        ollama list
        ```
        This command should list your installed models if Ollama is running and accessible.

6.  **Pull the LLM Model:** ğŸ§ 
    The RAG script is configured by default to use `llama3.2`. Pull this model using the Ollama CLI:
    ```bash
    ollama pull llama3.2
    ```
    Wait for the download to complete. You can verify the model is installed using `ollama list` again.
    *Note: If you wish to use a different model, update the `model` value within the Python script (likely in an `LLM_CONFIG` dictionary) and pull the corresponding model using `ollama pull <new_model_name>`.*

#### Running the RAG Application â–¶ï¸

Once the setup is complete and Ollama is running with the required model:
```bash
python RAG_example.py
```
Then, open your web browser and navigate to http://localhost:7860 ğŸŒ. You should see the Gradio interface where you can upload documents and interact with the RAG system.

# AI Application Suite ğŸš€

## 2. Simple Chat with Ollama (`simpleChat.py`) ğŸ’¬ğŸ¤–  
A minimalist command-line interface for interacting with locally hosted Ollama models. Perfect for:  
âœ… Learning LLM interaction fundamentals  
âœ… Testing model responses  
âœ… Prototyping without UI complexity  

---

## 3. Intelligent Link Extraction (`inference_with_knowledge.py`) ğŸ”—ğŸŒ  
Advanced web analysis tool featuring:  

1. **Website Scraping** - Automatically harvest all hyperlinks  
2. **AI Analysis** - Local Ollama-powered relevance scoring ğŸ§   
3. **Business Intelligence** - Identifies core business sections:  
   - About Us  
   - Services/Products  
   - Contact Information  
   - Company Values  

*Perfect for competitive analysis and website mapping!*

---


## ğŸš€ Usage Guide  

### Prerequisites  
ğŸ¦™ **Ollama Running**:  
```bash
ollama serve  # Keep running in background
